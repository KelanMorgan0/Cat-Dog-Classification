{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Cat-Dog Classification\n",
    "\n",
    "data received from [Kaggle Cats and Dogs Dataset](https://www.microsoft.com/en-us/download/details.aspx?id=54765)\n",
    "\n",
    "# Imports"
   ],
   "id": "98f5c9a389741f94"
  },
  {
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-03-23T20:15:48.682928Z",
     "start_time": "2025-03-23T20:15:42.353691Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder"
   ],
   "id": "initial_id",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-23T20:15:48.696462Z",
     "start_time": "2025-03-23T20:15:48.689938Z"
    }
   },
   "cell_type": "code",
   "source": "print(tf.__version__)",
   "id": "4809b0c0bbad1136",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.19.0\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Take in the images and label them dog and cat",
   "id": "aa67c30e2c955701"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-23T20:16:57.056229Z",
     "start_time": "2025-03-23T20:15:49.141565Z"
    }
   },
   "cell_type": "code",
   "source": [
    "X = []\n",
    "y = []\n",
    "\n",
    "# load all dog data\n",
    "for img_name in os.listdir(\"Data/Dog\"):\n",
    "    img_path = os.path.join(\"Data/Dog\", img_name)\n",
    "\n",
    "    #re-size images to 64 by 64\n",
    "    img = Image.open(img_path).convert(\"RGB\").resize((64, 64))\n",
    "    img = np.array(img) / 255.0\n",
    "\n",
    "    # add images to arrays\n",
    "    X.append(img)\n",
    "    y.append(\"Dog\")\n",
    "\n",
    "# load all cat data\n",
    "for img_name in os.listdir(\"Data/Cat\"):\n",
    "    img_path = os.path.join(\"Data/Cat\", img_name)\n",
    "\n",
    "    #re-size images to 64 by 64\n",
    "    img = Image.open(img_path).convert(\"RGB\").resize((64, 64))\n",
    "    img = np.array(img) / 255.0\n",
    "\n",
    "    # add images to arrays\n",
    "    X.append(img)\n",
    "    y.append(\"cat\")\n",
    "\n",
    "X = np.array(X)\n",
    "y = np.array(y)"
   ],
   "id": "da62ed9e3c352950",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kelan\\OneDrive\\Documents\\GitHub\\Cat-Dog-Classification\\venv\\Lib\\site-packages\\PIL\\TiffImagePlugin.py:949: UserWarning: Truncated File Read\n",
      "  warnings.warn(str(msg))\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Encode and split the data into test and training",
   "id": "e5fb97b252b2bf5f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-23T20:17:02.181341Z",
     "start_time": "2025-03-23T20:16:57.132576Z"
    }
   },
   "cell_type": "code",
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "# get validation data\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "y_train = label_encoder.fit_transform(y_train)\n",
    "y_val = label_encoder.fit_transform(y_val)"
   ],
   "id": "3a294dfa94bd7a86",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Create CNN Model",
   "id": "45826077fb06a130"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-23T20:17:02.712925Z",
     "start_time": "2025-03-23T20:17:02.200353Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Specify the number of filters and their sizes\n",
    "# Dropout randomly drops half the neurons form the dense call (prevents overfitting)\n",
    "# Dense 1 is classification of cat or dog\n",
    "\n",
    "model = Sequential([\n",
    "    Conv2D(32, (3, 3), activation='relu', input_shape=(64, 64, 3)),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    Conv2D(64, (3, 3), activation='relu'),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    Flatten(),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
   ],
   "id": "d184dfd51c29cc8d",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kelan\\OneDrive\\Documents\\GitHub\\Cat-Dog-Classification\\venv\\Lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Train the model\n",
    "- x_train = image data\n",
    "- y_train = labels\n",
    "- epochs = number of times trained"
   ],
   "id": "7dd59fc391e65fa5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-23T20:21:32.606259Z",
     "start_time": "2025-03-23T20:17:02.864906Z"
    }
   },
   "cell_type": "code",
   "source": "model = model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_val, y_val))",
   "id": "81568f8df46fb842",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001B[1m500/500\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m30s\u001B[0m 55ms/step - accuracy: 0.5638 - loss: 0.7012 - val_accuracy: 0.7048 - val_loss: 0.5792\n",
      "Epoch 2/10\n",
      "\u001B[1m500/500\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m25s\u001B[0m 49ms/step - accuracy: 0.7061 - loss: 0.5740 - val_accuracy: 0.7310 - val_loss: 0.5500\n",
      "Epoch 3/10\n",
      "\u001B[1m500/500\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m26s\u001B[0m 51ms/step - accuracy: 0.7547 - loss: 0.5067 - val_accuracy: 0.7765 - val_loss: 0.4766\n",
      "Epoch 4/10\n",
      "\u001B[1m500/500\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m26s\u001B[0m 52ms/step - accuracy: 0.7862 - loss: 0.4564 - val_accuracy: 0.7900 - val_loss: 0.4563\n",
      "Epoch 5/10\n",
      "\u001B[1m500/500\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m26s\u001B[0m 52ms/step - accuracy: 0.8130 - loss: 0.4076 - val_accuracy: 0.8060 - val_loss: 0.4367\n",
      "Epoch 6/10\n",
      "\u001B[1m500/500\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m27s\u001B[0m 55ms/step - accuracy: 0.8350 - loss: 0.3675 - val_accuracy: 0.8125 - val_loss: 0.4330\n",
      "Epoch 7/10\n",
      "\u001B[1m500/500\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m26s\u001B[0m 52ms/step - accuracy: 0.8767 - loss: 0.2792 - val_accuracy: 0.8092 - val_loss: 0.4437\n",
      "Epoch 9/10\n",
      "\u001B[1m500/500\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m28s\u001B[0m 55ms/step - accuracy: 0.8641 - loss: 0.3178 - val_accuracy: 0.8075 - val_loss: 0.4601\n",
      "Epoch 8/10\n",
      "\u001B[1m500/500\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m27s\u001B[0m 54ms/step - accuracy: 0.8970 - loss: 0.2391 - val_accuracy: 0.8008 - val_loss: 0.5003\n",
      "Epoch 10/10\n",
      "\u001B[1m500/500\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m27s\u001B[0m 53ms/step - accuracy: 0.9183 - loss: 0.2013 - val_accuracy: 0.8130 - val_loss: 0.5149\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-23T20:21:32.712304Z",
     "start_time": "2025-03-23T20:21:32.708514Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "9e670c825b3da129",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
